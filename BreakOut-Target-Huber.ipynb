{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models     import Sequential\n",
    "from tensorflow.keras.layers     import Dense, Conv2D, multiply, Input, Lambda, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from collections import deque \n",
    "from tensorflow.keras.models import load_model, clone_model\n",
    "from gc import collect\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from time import time, sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DQ:\n",
    "    def __init__(self, state_size, action_size,depth ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.depth = depth\n",
    "        self.build_model()\n",
    "        #self.model.save('tmp_model')\n",
    "        #self.guesser = load_model('tmp_model')\n",
    "        self.guesser = clone_model(self.model)\n",
    "        self.update_guesser()\n",
    "        #print(self.model.get_weights())\n",
    "        #print(self.guesser.get_weights())\n",
    "    def huber_loss(self,a, b, in_keras=True):\n",
    "        error = a - b\n",
    "        quadratic_term = error*error / 2\n",
    "        linear_term = abs(error) - 1/2\n",
    "        use_linear_term = (abs(error) > 1.0)\n",
    "        if in_keras:\n",
    "            # Keras won't let us multiply floats by booleans, so we explicitly cast the booleans to floats\n",
    "            use_linear_term = K.cast(use_linear_term, 'float32')\n",
    "        return use_linear_term * linear_term + (1-use_linear_term) * quadratic_term\n",
    "    def build_model(self):\n",
    "        ATARI_SHAPE = (210, 160, 3)\n",
    "\n",
    "        # With the functional API we need to define the inputs.\n",
    "        frames_input = Input(ATARI_SHAPE, name='frames')\n",
    "        actions_input = Input((self.action_size,), name='mask')\n",
    "\n",
    "        # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "        normalized = Lambda(lambda x: x / 255.0)(frames_input)\n",
    "\n",
    "        # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "        conv_1 = Conv2D(16, kernel_size=(8,8), strides=(4,4), activation='relu')(normalized)\n",
    "        # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "        conv_2 = Conv2D(32, kernel_size=(4,4), strides=(2,2), activation='relu')(conv_1)\n",
    "        # Flattening the second convolutional layer.\n",
    "        conv_flattened =Flatten()(conv_2)\n",
    "        # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "        hidden = Dense(256, activation='relu')(conv_flattened)\n",
    "        # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "        output = Dense(self.action_size)(hidden)\n",
    "        # Finally, we multiply the output by the mask!\n",
    "        filtered_output = multiply([output, actions_input])\n",
    "\n",
    "        self.model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "        optimizer = RMSprop(lr=0.0005, rho=0.95, epsilon=0.01)\n",
    "        self.model.compile(optimizer, loss=self.huber_loss)\n",
    "\n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "        \n",
    "    def act(self,state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = np.argmax(self.model.predict([state,self.mask()])[0])\n",
    "        return act_values\n",
    "    def mask(self,action = -1):\n",
    "        if not action in range(self.action_size):\n",
    "            return np.reshape(np.ones((self.action_size,)),(1,self.action_size,))\n",
    "        full_mask = np.zeros((4,))\n",
    "        full_mask[action] = 1\n",
    "        full_mask = np.reshape(full_mask,(1,self.action_size,))\n",
    "        return np.array(full_mask)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def update_guesser(self):\n",
    "        #self.model.save('tmp_model')\n",
    "        #self.guesser = load_model('tmp_model')\n",
    "        #collect()\n",
    "        self.guesser.set_weights(self.model.get_weights()) \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                act = self.mask(action)\n",
    "                #print(act.shape)\n",
    "                target = reward + self.gamma *np.amax(self.guesser.predict([next_state,act])[0])\n",
    "            target_f = self.guesser.predict([state,self.mask(action)])\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit([state,self.mask(action)], target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/20000, score: 210\n",
      "Run Time: 0.002 hrs\n",
      "Time Left: 46.862 hrs\n"
     ]
    },
    {
     " "
     ]
    }
   ],
   "source": [
    "class CartPole:\n",
    "    def __init__(self,depth):\n",
    "        self.depth = depth\n",
    "        self.game_name = 'BreakoutDeterministic-v4'\n",
    "        self.env = gym.make(self.game_name)\n",
    "        self.state_size = self.env.observation_space.shape\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.scores = []  \n",
    "        self.network_runs = []\n",
    "        self.time_predictions = []\n",
    "        self.time_stamps = []\n",
    "        self.start = time()\n",
    "        self.agent = DQ(self.state_size, self.action_size, self.depth)\n",
    "        self.resized = [1]\n",
    "        self.update_length = 200\n",
    "        for i in self.state_size:\n",
    "            self.resized.append(i)\n",
    "        \n",
    "    def train(self,episodes):\n",
    "        self.env.reset()\n",
    "        \n",
    "        for e in range(episodes):\n",
    "            #print('a')\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, self.resized)\n",
    "            time_t = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                time_t+=1\n",
    "                #print('a')\n",
    "                #self.env.render()\n",
    "                action = self.agent.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                #reward = reward if not done else -.5\n",
    "                next_state = np.reshape(next_state, self.resized)\n",
    "                self.agent.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "\n",
    "                #if done and e % 20 == 0 and not e == 0:\n",
    "        \n",
    "            # print the score and break out of the loop\n",
    "            if e % self.update_length == 1:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episodes, time_t))\n",
    "                time_per = (time()-self.start)/(e)\n",
    "                time_pre = (time_per*(episodes-e))/3600\n",
    "                print(\"Run Time:\",'%.3f'%((time()-self.start)/3600),'hrs')\n",
    "                print(\"Time Left:\", '%.3f'%float(time_pre), 'hrs')\n",
    "                self.time_stamps.append('%.3f'%((time()-self.start)/3600))\n",
    "                self.time_predictions.append(time_pre)\n",
    "                self.agent.update_guesser()\n",
    "            self.scores.append(time_t)\n",
    "\n",
    "        \n",
    "            \n",
    "            if(len(self.agent.memory) > 32):\n",
    "                self.agent.replay(32)\n",
    "        self.env.close()\n",
    "        \n",
    "    def evaluate(self,games = 1,frame_rate = 6):\n",
    "        \n",
    "        completed = 0\n",
    "        env = gym.make(self.game_name)\n",
    "        state = env.reset()\n",
    "        state = np.resize(state, self.resized)\n",
    "        while completed < games:\n",
    "            start = time()\n",
    "            env.render()\n",
    "            action = self.agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #reward = reward if not done else -.5\n",
    "            next_state = np.reshape(next_state, self.resized)\n",
    "            self.agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                completed += 1\n",
    "                env.reset()\n",
    "            sleep(1/frame_rate-(time()-start))\n",
    "        env.close()\n",
    "        \n",
    "carts = []\n",
    "#for depth in [2,12,24,64]:\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    # Setup operations\n",
    "    for depth in [32]:\n",
    "        carts.append(CartPole(depth))\n",
    "        carts[-1].train(20000)\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    # Run your code\n",
    "    carts[0].evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CartPole_Grapher:\n",
    "    def __init__(self,carts):\n",
    "        self.carts = carts\n",
    "        self.max = 500\n",
    "    def moving_avg(self,lis,tail_p):\n",
    "        curr = 0\n",
    "        tail = max([1,int(len(lis)*tail_p)])\n",
    "        m_lis = []\n",
    "        for i in range(len(lis)):\n",
    "            if i >= tail:\n",
    "                if i > 0:\n",
    "                    curr *= tail\n",
    "                    curr -= lis[i-tail]\n",
    "                    curr += lis[i]\n",
    "                    curr /= tail\n",
    "                else:\n",
    "                    curr = lis[i]\n",
    "\n",
    "            else:\n",
    "                if i > 0:\n",
    "                    curr *= i-1\n",
    "                    curr += lis[i]\n",
    "                    curr /= i\n",
    "                else:\n",
    "                    curr = lis[i]\n",
    "            m_lis.append(curr)\n",
    "        return m_lis\n",
    "    def sub_graph(self, tail_p, names):\n",
    "        plot = 0\n",
    "        rows = np.ceil(len(carts)/2)\n",
    "        for cart in self.carts:\n",
    "            \n",
    "            plt.subplot(2,2,plot+1)\n",
    "            plt.axis([0, len(cart.scores), 0, self.max])\n",
    "            plt.plot(self.moving_avg(cart.scores,tail_p))\n",
    "            plt.title(names[plot])\n",
    "            plt.figure(figsize=(20,10))\n",
    "            \n",
    "            \n",
    "            plot += 1\n",
    "        plt.show()  \n",
    "        \n",
    "    def graph(self, tail_p, names):\n",
    "        plot = 0\n",
    "        rows = np.ceil(len(carts)/2)\n",
    "        for cart in self.carts:\n",
    "            \n",
    "            plt.axis([0, len(cart.scores), 0, self.max])\n",
    "            plt.plot(self.moving_avg(cart.scores,tail_p))\n",
    "            plt.title(names[plot])\n",
    "            plt.figure(figsize=(20,10))\n",
    "\n",
    "            plot +=1\n",
    "            plt.show()\n",
    "    def evaluate(self,games = 1):\n",
    "        completed = 0\n",
    "        env = gym.make(self.game_name)\n",
    "        state = env.reset()\n",
    "        state = np.resize(state, [1, self.state_size])\n",
    "        while completed < games:\n",
    "            start = time()\n",
    "            env.render()\n",
    "            action = self.agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #reward = reward if not done else -.5\n",
    "            next_state = np.reshape(next_state, [1, self.state_size])\n",
    "            self.agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                completed += 1\n",
    "                env.reset()\n",
    "            sleep(.1-(time()-start))\n",
    "        env.close()\n",
    "            \n",
    "cpg = CartPole_Grapher(carts)\n",
    "cpg.graph(.05, names = [size + ' Network' for size in ['Tiny', 'Small', 'Medium', 'Large']])\n",
    "#cpg.graph(.01, names = [size + ' Network' for size in ['Medium']])\n",
    "cpg.chunk_graph(200, names = [size + ' Network' for size in ['Tiny', 'Small', 'Medium', 'Large']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "temp = 0\n",
    "ascores = []\n",
    "avg = 50\n",
    "for i in range(len(scores2)):\n",
    "    if i%avg == 0 and i != 0:\n",
    "        ascores.append(temp/avg)\n",
    "        temp = 0\n",
    "    else:\n",
    "        temp += scores2[i]\n",
    " \n",
    "def plot(x):\n",
    "    plt.plot(range(len(x)),x)\n",
    "    plt.show()\n",
    "plot(ascores)'''\n",
    "\n",
    "\n",
    "'''depths = ['Large','Medium','Small']\n",
    "training_length = ['Short','Medium','Long']\n",
    "avg = 100\n",
    "plots = []\n",
    "plt.figure(figsize=(20,10))\n",
    "for run in network_runs:\n",
    "    temp_plot = []\n",
    "    temp = 0\n",
    "    for i in range(int(len(run))):\n",
    "        temp += run[i]\n",
    "        if i % avg == 0:\n",
    "            temp_plot.append(temp/avg)\n",
    "            temp = 0\n",
    "    plots.append(temp_plot)\n",
    "for plot in range(len(plots)):\n",
    "    plt.subplot(3, 3, plot+1)\n",
    "    plt.plot(plots[plot])\n",
    "    #title = depths[run%3]+' Nework with '+training_length[int(run/3)]+' Training Period'\n",
    "    title = depths[plot%3]+' Nework'\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Training_Batches')\n",
    "plt.show()  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
