{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models     import Sequential\n",
    "from tensorflow.keras.layers     import Dense, Conv2D, multiply, Input, Lambda, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from collections import deque \n",
    "from tensorflow.keras.models import load_model, clone_model\n",
    "from gc import collect\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from time import time, sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DQ:\n",
    "    def __init__(self, state_size, action_size,depth ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.depth = depth\n",
    "        self.build_model()\n",
    "        #self.model.save('tmp_model')\n",
    "        #self.guesser = load_model('tmp_model')\n",
    "        self.guesser = clone_model(self.model)\n",
    "        self.update_guesser()\n",
    "        #print(self.model.get_weights())\n",
    "        #print(self.guesser.get_weights())\n",
    "    def huber_loss(self,a, b, in_keras=True):\n",
    "        error = a - b\n",
    "        quadratic_term = error*error / 2\n",
    "        linear_term = abs(error) - 1/2\n",
    "        use_linear_term = (abs(error) > 1.0)\n",
    "        if in_keras:\n",
    "            # Keras won't let us multiply floats by booleans, so we explicitly cast the booleans to floats\n",
    "            use_linear_term = K.cast(use_linear_term, 'float32')\n",
    "        return use_linear_term * linear_term + (1-use_linear_term) * quadratic_term\n",
    "    def build_model(self):\n",
    "        ATARI_SHAPE = (210, 160, 3)\n",
    "\n",
    "        # With the functional API we need to define the inputs.\n",
    "        frames_input = Input(ATARI_SHAPE, name='frames')\n",
    "        actions_input = Input((self.action_size,), name='mask')\n",
    "\n",
    "        # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "        normalized = Lambda(lambda x: x / 255.0)(frames_input)\n",
    "\n",
    "        # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "        conv_1 = Conv2D(16, kernel_size=(8,8), strides=(4,4), activation='relu')(normalized)\n",
    "        # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "        conv_2 = Conv2D(32, kernel_size=(4,4), strides=(2,2), activation='relu')(conv_1)\n",
    "        # Flattening the second convolutional layer.\n",
    "        conv_flattened =Flatten()(conv_2)\n",
    "        # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "        hidden = Dense(256, activation='relu')(conv_flattened)\n",
    "        # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "        output = Dense(self.action_size)(hidden)\n",
    "        # Finally, we multiply the output by the mask!\n",
    "        filtered_output = multiply([output, actions_input])\n",
    "\n",
    "        self.model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "        optimizer = RMSprop(lr=0.0005, rho=0.95, epsilon=0.01)\n",
    "        self.model.compile(optimizer, loss=self.huber_loss)\n",
    "\n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "        \n",
    "    def act(self,state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = np.argmax(self.model.predict([state,self.mask()])[0])\n",
    "        return act_values\n",
    "    def mask(self,action = -1):\n",
    "        if not action in range(self.action_size):\n",
    "            return np.reshape(np.ones((self.action_size,)),(1,self.action_size,))\n",
    "        full_mask = np.zeros((4,))\n",
    "        full_mask[action] = 1\n",
    "        full_mask = np.reshape(full_mask,(1,self.action_size,))\n",
    "        return np.array(full_mask)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def update_guesser(self):\n",
    "        #self.model.save('tmp_model')\n",
    "        #self.guesser = load_model('tmp_model')\n",
    "        #collect()\n",
    "        self.guesser.set_weights(self.model.get_weights()) \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                act = self.mask(action)\n",
    "                #print(act.shape)\n",
    "                target = reward + self.gamma *np.amax(self.guesser.predict([next_state,act])[0])\n",
    "            target_f = self.guesser.predict([state,self.mask(action)])\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit([state,self.mask(action)], target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/20000, score: 210\n",
      "Run Time: 0.002 hrs\n",
      "Time Left: 46.862 hrs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-406c2b574564>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mcarts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCartPole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mcarts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_device_placement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;31m# Run your code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-406c2b574564>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, episodes)\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;31m#print('a')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[1;31m#self.env.render()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;31m#reward = reward if not done else -.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-88ef29a8c722>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mact_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mact_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nueral\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1876\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m       return training_arrays.predict_loop(\n\u001b[1;32m-> 1878\u001b[1;33m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nueral\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, inputs, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nueral\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nueral\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class CartPole:\n",
    "    def __init__(self,depth):\n",
    "        self.depth = depth\n",
    "        self.game_name = 'BreakoutDeterministic-v4'\n",
    "        self.env = gym.make(self.game_name)\n",
    "        self.state_size = self.env.observation_space.shape\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.scores = []  \n",
    "        self.network_runs = []\n",
    "        self.time_predictions = []\n",
    "        self.time_stamps = []\n",
    "        self.start = time()\n",
    "        self.agent = DQ(self.state_size, self.action_size, self.depth)\n",
    "        self.resized = [1]\n",
    "        self.update_length = 200\n",
    "        for i in self.state_size:\n",
    "            self.resized.append(i)\n",
    "        \n",
    "    def train(self,episodes):\n",
    "        self.env.reset()\n",
    "        \n",
    "        for e in range(episodes):\n",
    "            #print('a')\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, self.resized)\n",
    "            time_t = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                time_t+=1\n",
    "                #print('a')\n",
    "                #self.env.render()\n",
    "                action = self.agent.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                #reward = reward if not done else -.5\n",
    "                next_state = np.reshape(next_state, self.resized)\n",
    "                self.agent.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "\n",
    "                #if done and e % 20 == 0 and not e == 0:\n",
    "        \n",
    "            # print the score and break out of the loop\n",
    "            if e % self.update_length == 1:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episodes, time_t))\n",
    "                time_per = (time()-self.start)/(e)\n",
    "                time_pre = (time_per*(episodes-e))/3600\n",
    "                print(\"Run Time:\",'%.3f'%((time()-self.start)/3600),'hrs')\n",
    "                print(\"Time Left:\", '%.3f'%float(time_pre), 'hrs')\n",
    "                self.time_stamps.append('%.3f'%((time()-self.start)/3600))\n",
    "                self.time_predictions.append(time_pre)\n",
    "                self.agent.update_guesser()\n",
    "            self.scores.append(time_t)\n",
    "\n",
    "        \n",
    "            \n",
    "            if(len(self.agent.memory) > 32):\n",
    "                self.agent.replay(32)\n",
    "        self.env.close()\n",
    "        \n",
    "    def evaluate(self,games = 1,frame_rate = 6):\n",
    "        \n",
    "        completed = 0\n",
    "        env = gym.make(self.game_name)\n",
    "        state = env.reset()\n",
    "        state = np.resize(state, self.resized)\n",
    "        while completed < games:\n",
    "            start = time()\n",
    "            env.render()\n",
    "            action = self.agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #reward = reward if not done else -.5\n",
    "            next_state = np.reshape(next_state, self.resized)\n",
    "            self.agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                completed += 1\n",
    "                env.reset()\n",
    "            sleep(1/frame_rate-(time()-start))\n",
    "        env.close()\n",
    "        \n",
    "carts = []\n",
    "#for depth in [2,12,24,64]:\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    # Setup operations\n",
    "    for depth in [32]:\n",
    "        carts.append(CartPole(depth))\n",
    "        carts[-1].train(20000)\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    # Run your code\n",
    "    carts[0].evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CartPole_Grapher:\n",
    "    def __init__(self,carts):\n",
    "        self.carts = carts\n",
    "        self.max = 500\n",
    "    def moving_avg(self,lis,tail_p):\n",
    "        curr = 0\n",
    "        tail = max([1,int(len(lis)*tail_p)])\n",
    "        m_lis = []\n",
    "        for i in range(len(lis)):\n",
    "            if i >= tail:\n",
    "                if i > 0:\n",
    "                    curr *= tail\n",
    "                    curr -= lis[i-tail]\n",
    "                    curr += lis[i]\n",
    "                    curr /= tail\n",
    "                else:\n",
    "                    curr = lis[i]\n",
    "\n",
    "            else:\n",
    "                if i > 0:\n",
    "                    curr *= i-1\n",
    "                    curr += lis[i]\n",
    "                    curr /= i\n",
    "                else:\n",
    "                    curr = lis[i]\n",
    "            m_lis.append(curr)\n",
    "        return m_lis\n",
    "    def sub_graph(self, tail_p, names):\n",
    "        plot = 0\n",
    "        rows = np.ceil(len(carts)/2)\n",
    "        for cart in self.carts:\n",
    "            \n",
    "            plt.subplot(2,2,plot+1)\n",
    "            plt.axis([0, len(cart.scores), 0, self.max])\n",
    "            plt.plot(self.moving_avg(cart.scores,tail_p))\n",
    "            plt.title(names[plot])\n",
    "            plt.figure(figsize=(20,10))\n",
    "            \n",
    "            \n",
    "            plot += 1\n",
    "        plt.show()  \n",
    "        \n",
    "    def graph(self, tail_p, names):\n",
    "        plot = 0\n",
    "        rows = np.ceil(len(carts)/2)\n",
    "        for cart in self.carts:\n",
    "            \n",
    "            plt.axis([0, len(cart.scores), 0, self.max])\n",
    "            plt.plot(self.moving_avg(cart.scores,tail_p))\n",
    "            plt.title(names[plot])\n",
    "            plt.figure(figsize=(20,10))\n",
    "\n",
    "            plot +=1\n",
    "            plt.show()\n",
    "    def evaluate(self,games = 1):\n",
    "        completed = 0\n",
    "        env = gym.make(self.game_name)\n",
    "        state = env.reset()\n",
    "        state = np.resize(state, [1, self.state_size])\n",
    "        while completed < games:\n",
    "            start = time()\n",
    "            env.render()\n",
    "            action = self.agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #reward = reward if not done else -.5\n",
    "            next_state = np.reshape(next_state, [1, self.state_size])\n",
    "            self.agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                completed += 1\n",
    "                env.reset()\n",
    "            sleep(.1-(time()-start))\n",
    "        env.close()\n",
    "            \n",
    "cpg = CartPole_Grapher(carts)\n",
    "cpg.graph(.05, names = [size + ' Network' for size in ['Tiny', 'Small', 'Medium', 'Large']])\n",
    "#cpg.graph(.01, names = [size + ' Network' for size in ['Medium']])\n",
    "cpg.chunk_graph(200, names = [size + ' Network' for size in ['Tiny', 'Small', 'Medium', 'Large']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "temp = 0\n",
    "ascores = []\n",
    "avg = 50\n",
    "for i in range(len(scores2)):\n",
    "    if i%avg == 0 and i != 0:\n",
    "        ascores.append(temp/avg)\n",
    "        temp = 0\n",
    "    else:\n",
    "        temp += scores2[i]\n",
    " \n",
    "def plot(x):\n",
    "    plt.plot(range(len(x)),x)\n",
    "    plt.show()\n",
    "plot(ascores)'''\n",
    "\n",
    "\n",
    "'''depths = ['Large','Medium','Small']\n",
    "training_length = ['Short','Medium','Long']\n",
    "avg = 100\n",
    "plots = []\n",
    "plt.figure(figsize=(20,10))\n",
    "for run in network_runs:\n",
    "    temp_plot = []\n",
    "    temp = 0\n",
    "    for i in range(int(len(run))):\n",
    "        temp += run[i]\n",
    "        if i % avg == 0:\n",
    "            temp_plot.append(temp/avg)\n",
    "            temp = 0\n",
    "    plots.append(temp_plot)\n",
    "for plot in range(len(plots)):\n",
    "    plt.subplot(3, 3, plot+1)\n",
    "    plt.plot(plots[plot])\n",
    "    #title = depths[run%3]+' Nework with '+training_length[int(run/3)]+' Training Period'\n",
    "    title = depths[plot%3]+' Nework'\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Training_Batches')\n",
    "plt.show()  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x1 = np.linspace(0.0, 5.0)\n",
    "x2 = np.linspace(0.0, 2.0)\n",
    "\n",
    "y1 = np.cos(2 * np.pi * x1) * np.exp(-x1)\n",
    "y2 = np.cos(2 * np.pi * x2)\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x1, y1, 'o-')\n",
    "plt.title('A tale of 2 subplots')\n",
    "plt.ylabel('Damped oscillation')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x2, y2, '.-')\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('Undamped')\n",
    "\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(carts[0].time_stamps)\n",
    "plt.show()\n",
    "plt.plot(carts[0].time_predictions)\n",
    "plt.show()\n",
    "error = []\n",
    "for i in range(len(carts[0].time_stamps)):\n",
    "    error.append((float(carts[0].time_stamps[-1])-float(carts[0].time_predictions[i])-float(carts[0].time_stamps[i]))/float(carts[0].time_stamps[-1]))\n",
    "plt.plot(error)\n",
    "plt.show()\n",
    "error_short = error[30:]\n",
    "for i in error_short:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'BreakoutDeterministic-v4'\n",
    "env = gym.make(game_name)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "print(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
